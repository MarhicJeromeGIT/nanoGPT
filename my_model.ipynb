{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.first_layer = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MyModel()\n",
    "\n",
    "# Train the model:\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss() # or any other appropriate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(1.1260e-12, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1000 times\n",
    "loss = 0\n",
    "for i in range(1000):\n",
    "  x = torch.rand(32, 10) # 32: batch size\n",
    "  target = torch.full((32,10), 1.0)\n",
    "  y = m(x)\n",
    "\n",
    "  loss = loss_function(y, target)\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print('loss : ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the weights:\n",
    "torch.save(m.state_dict(), 'my_model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelLora(MyModel):\n",
    "    def __init__(self):\n",
    "        super().__init__() # Call the base class's __init__ method\n",
    "        # Additional initialization logic for MyModel2\n",
    "        self.lora_A = nn.Linear(10, 1)\n",
    "        self.lora_B = nn.Linear(1, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Custom forward pass logic here\n",
    "        x1 = super().forward(x)\n",
    "        x2 = self.lora_A(x) # -> [10 * 1]\n",
    "        x2 = self.lora_B(x2) # [10 * 10]\n",
    "\n",
    "        return (x1 + x2)\n",
    "  \n",
    "    def freeze_parent_weights(self):\n",
    "        for name, param in self.first_layer.named_parameters():\n",
    "            print('freezing first_layer.', name)\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1110, 0.0508, 1.2207, 1.2921, 1.4932, 1.7057, 0.2008, 0.4966, 1.5487,\n",
      "         1.5557]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# load the model from weights:\n",
    "m2 = MyModelLora()\n",
    "m2.load_state_dict(torch.load('my_model_state_dict.pth'), strict=False)\n",
    "\n",
    "x = torch.rand(1, 10)\n",
    "y = m2(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing first_layer. weight\n",
      "freezing first_layer. bias\n"
     ]
    }
   ],
   "source": [
    "# freeze the weights:\n",
    "m2.freeze_parent_weights()\n",
    "\n",
    "# Add new weights:\n",
    "\n",
    "optimizer = torch.optim.Adam(m2.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss() # or any other appropriate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(1.8874e-08, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# try it out:\n",
    "loss = 0\n",
    "for i in range(1000):\n",
    "  x = torch.rand(32, 10) # 32: batch size\n",
    "  target = torch.full((32, 10), 0.8)\n",
    "  y = m2(x)\n",
    "\n",
    "  loss = loss_function(y, target)\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print('loss : ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8003]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2000, -0.2001, -0.1999, -0.1999, -0.2000, -0.1999, -0.2000, -0.2000,\n",
      "         -0.1999, -0.1999]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 10)\n",
    "x1 = m2.lora_A(x) # -> [0.8]\n",
    "print(x1)\n",
    "x2 = m2.lora_B(x1) # -> [-0.2, -0.2] ...\n",
    "print(x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(m2.lora_B.weight.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# let's count the params:\n",
    "# original model:\n",
    "count_1 = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print(count_1)\n",
    "\n",
    "# lora:\n",
    "count_2 = sum(p.numel() for p in m2.parameters() if p.requires_grad)\n",
    "print(count_2) # loraA: 10+1 elem loraB: 10+10 elems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
